#!/usr/bin/env python3
"""
Comprehensive validation script for PromptShield v1.1
Tests all features including drift detection, baseline management, and full workflow
"""

import sys
import os
import requests
import time
import json
from typing import Dict, Any, Optional

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# API base URL
API_BASE = os.getenv("API_BASE_URL", "http://localhost:8000/api/v1")

# Colors for output
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    RESET = '\033[0m'
    BOLD = '\033[1m'

def print_success(msg):
    print(f"{Colors.GREEN}âœ… {msg}{Colors.RESET}")

def print_error(msg):
    print(f"{Colors.RED}âŒ {msg}{Colors.RESET}")

def print_warning(msg):
    print(f"{Colors.YELLOW}âš ï¸  {msg}{Colors.RESET}")

def print_info(msg):
    print(f"{Colors.BLUE}â„¹ï¸  {msg}{Colors.RESET}")

def print_header(msg):
    print(f"\n{Colors.BOLD}{'=' * 70}{Colors.RESET}")
    print(f"{Colors.BOLD}{msg}{Colors.RESET}")
    print(f"{Colors.BOLD}{'=' * 70}{Colors.RESET}\n")

def check_server():
    """Check if backend server is running"""
    try:
        base_url = API_BASE.replace('/api/v1', '')
        response = requests.get(f"{base_url}/health", timeout=5)
        if response.status_code == 200:
            print_success("Backend server is running")
            return True
        else:
            print_error(f"Backend server returned status {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print_error("Cannot connect to backend server. Is it running on http://localhost:8000?")
        print_info("Start the server with: cd backend && uvicorn main:app --reload")
        return False
    except Exception as e:
        print_error(f"Error checking server: {e}")
        return False

def validate_database():
    """Validate database tables exist"""
    print_header("1. Database Validation")
    try:
        from app.db.database import SessionLocal, engine
        from app.db.models import Baseline, Embedding, DriftResult, AgentTrace, Execution, Result, Pipeline, LLMConfig
        from sqlalchemy import inspect
        
        inspector = inspect(engine)
        tables = inspector.get_table_names()
        required = ['baselines', 'embeddings', 'drift_results', 'agent_traces', 'executions', 'results', 'pipelines', 'llm_configs']
        
        all_ok = True
        for table in required:
            if table in tables:
                print_success(f"Table '{table}' exists")
            else:
                print_error(f"Table '{table}' is MISSING")
                all_ok = False
        
        return all_ok
    except Exception as e:
        print_error(f"Database validation failed: {e}")
        return False

def validate_models():
    """Validate models can be imported and used"""
    print_header("2. Models Validation")
    try:
        from app.db.models import (
            Baseline, Embedding, DriftResult, AgentTrace,
            Execution, Result, Pipeline, LLMConfig
        )
        print_success("All models importable")
        
        # Check model attributes
        models_to_check = [
            (Baseline, ['id', 'execution_id', 'name', 'baseline_tag']),
            (Embedding, ['id', 'result_id', 'execution_id', 'embedding_vector', 'model_name']),
            (DriftResult, ['id', 'execution_id', 'baseline_execution_id', 'drift_type', 'metric', 'value']),
            (AgentTrace, ['id', 'execution_id', 'step', 'action_type', 'tool_name']),
        ]
        
        for model, attrs in models_to_check:
            for attr in attrs:
                if hasattr(model, attr):
                    print_success(f"{model.__name__}.{attr}")
                else:
                    print_error(f"{model.__name__}.{attr} - MISSING")
                    return False
        
        return True
    except Exception as e:
        print_error(f"Models validation failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def validate_services():
    """Validate services can be imported"""
    print_header("3. Services Validation")
    services = [
        ('app.services.baseline_manager', 'BaselineManager'),
        ('app.services.embedding_generator', 'EmbeddingGenerator'),
        ('app.services.agent_trace_extractor', 'AgentTraceExtractor'),
        ('app.services.execution_engine', 'ExecutionEngine'),
        ('app.services.library_adapters', 'LibraryAdapter'),
    ]
    
    all_ok = True
    for module_path, class_name in services:
        try:
            module = __import__(module_path, fromlist=[class_name])
            cls = getattr(module, class_name)
            print_success(f"{module_path}.{class_name}")
        except Exception as e:
            print_error(f"{module_path}.{class_name} - {e}")
            all_ok = False
    
    # Check DriftEngine separately (may have circular import issues)
    try:
        from app.services.drift_engine import DriftEngine
        print_success("app.services.drift_engine.DriftEngine")
    except Exception as e:
        print_warning(f"app.services.drift_engine.DriftEngine - {e}")
        print_warning("  (This may be OK if drift_engine is only used in API endpoints)")
        # Don't fail validation for this
    
    return all_ok

def validate_config():
    """Validate configuration"""
    print_header("4. Configuration Validation")
    try:
        from app.core.config import settings
        
        checks = [
            ('EMBEDDING_MODEL', settings.EMBEDDING_MODEL),
            ('DRIFT_THRESHOLDS', settings.DRIFT_THRESHOLDS),
            ('ENABLE_AGENT_TRACES', settings.ENABLE_AGENT_TRACES),
        ]
        
        all_ok = True
        for name, value in checks:
            if value is not None:
                print_success(f"{name} = {value}")
            else:
                print_error(f"{name} is None")
                all_ok = False
        
        # Check drift thresholds structure
        if isinstance(settings.DRIFT_THRESHOLDS, dict):
            required_keys = ['output', 'safety', 'distribution', 'embedding', 'agent_tool']
            for key in required_keys:
                if key in settings.DRIFT_THRESHOLDS:
                    print_success(f"DRIFT_THRESHOLDS['{key}'] = {settings.DRIFT_THRESHOLDS[key]}")
                else:
                    print_error(f"DRIFT_THRESHOLDS['{key}'] - MISSING")
                    all_ok = False
        
        return all_ok
    except Exception as e:
        print_error(f"Configuration validation failed: {e}")
        return False

def test_api_endpoint(method: str, endpoint: str, data: Optional[Dict] = None, expected_status: int = 200) -> Optional[Dict]:
    """Test an API endpoint"""
    url = f"{API_BASE}{endpoint}"
    try:
        if method.upper() == "GET":
            response = requests.get(url, timeout=10)
        elif method.upper() == "POST":
            response = requests.post(url, json=data, timeout=10)
        elif method.upper() == "DELETE":
            response = requests.delete(url, timeout=10)
        else:
            print_error(f"Unsupported method: {method}")
            return None
        
        if response.status_code == expected_status:
            print_success(f"{method} {endpoint} - Status {response.status_code}")
            try:
                return response.json()
            except:
                return {"status": "ok"}
        else:
            print_error(f"{method} {endpoint} - Expected {expected_status}, got {response.status_code}")
            try:
                error_detail = response.json()
                print_error(f"  Error: {error_detail}")
            except:
                print_error(f"  Response: {response.text[:200]}")
            return None
    except requests.exceptions.RequestException as e:
        print_error(f"{method} {endpoint} - Request failed: {e}")
        return None

def validate_api_endpoints():
    """Validate API endpoints are accessible"""
    print_header("5. API Endpoints Validation")
    
    # Health check (at root level, not under /api/v1)
    try:
        base_url = API_BASE.replace('/api/v1', '')
        response = requests.get(f"{base_url}/health", timeout=5)
        if response.status_code == 200:
            print_success("GET /health - Status 200")
        else:
            print_error(f"GET /health - Expected 200, got {response.status_code}")
            return False
    except Exception as e:
        print_error(f"GET /health - Request failed: {e}")
        return False
    
    # Test basic endpoints
    endpoints = [
        ("GET", "/llm-configs"),
        ("GET", "/pipelines"),
        ("GET", "/executions/"),
        ("GET", "/baselines"),
    ]
    
    all_ok = True
    for method, endpoint, *rest in endpoints:
        expected = rest[0] if rest else 200
        result = test_api_endpoint(method, endpoint, expected_status=expected)
        if result is None and expected == 200:
            all_ok = False
    
    # Test drift endpoint (should return empty array or 404 for non-existent execution)
    drift_result = test_api_endpoint("GET", "/drift/execution/99999", expected_status=200)
    if drift_result is None:
        # Try 404 as alternative
        drift_result_404 = test_api_endpoint("GET", "/drift/execution/99999", expected_status=404)
        if drift_result_404 is None:
            print_warning("Drift endpoint test inconclusive (both 200 and 404 failed)")
        else:
            print_success("Drift endpoint returns 404 for non-existent execution (expected behavior)")
    
    return all_ok

def test_full_workflow():
    """Test complete workflow: Config -> Pipeline -> Execution -> Results -> Baseline -> Drift"""
    print_header("6. Full Workflow Test")
    
    # Step 1: Create LLM Config
    print_info("Step 1: Creating LLM Config...")
    llm_config_data = {
        "name": "Test LLM Config - Validation",
        "endpoint_url": "http://localhost:11434/v1/chat/completions",
        "method": "POST",
        "headers": {
            "Content-Type": "application/json"
        },
        "payload_template": json.dumps({
            "model": "llama2",
            "messages": [
                {"role": "system", "content": "{system_prompt}"},
                {"role": "user", "content": "{prompt}"}
            ],
            "temperature": 0.7
        }),
        "environment": "test"
    }
    
    llm_config = test_api_endpoint("POST", "/llm-configs", llm_config_data)
    if not llm_config:
        print_error("Failed to create LLM config")
        return False
    
    llm_config_id = llm_config.get("id")
    print_success(f"Created LLM Config ID: {llm_config_id}")
    
    # Step 2: Create Pipeline
    print_info("Step 2: Creating Pipeline...")
    pipeline_data = {
        "name": "Test Pipeline - Validation",
        "description": "Validation test pipeline",
        "libraries": ["garak", "pyrit"],
        "test_categories": ["prompt_injection", "jailbreak"],
        "severity_thresholds": {},
        "llm_config_id": llm_config_id,
        "is_template": False
    }
    
    pipeline = test_api_endpoint("POST", "/pipelines", pipeline_data)
    if not pipeline:
        print_error("Failed to create pipeline")
        return False
    
    pipeline_id = pipeline.get("id")
    print_success(f"Created Pipeline ID: {pipeline_id}")
    
    # Step 3: Create Execution
    print_info("Step 3: Creating Execution...")
    execution_data = {
        "pipeline_id": pipeline_id,
        "llm_config_id": llm_config_id
    }
    
    execution = test_api_endpoint("POST", "/executions/", execution_data)
    if not execution:
        print_error("Failed to create execution")
        return False
    
    execution_id = execution.get("id")
    print_success(f"Created Execution ID: {execution_id}")
    
    # Step 4: Wait for execution to complete
    print_info("Step 4: Waiting for execution to complete...")
    max_wait = 120  # 2 minutes
    wait_time = 0
    execution_status = "pending"
    
    while wait_time < max_wait:
        time.sleep(5)
        wait_time += 5
        exec_response = test_api_endpoint("GET", f"/executions/{execution_id}")
        if exec_response:
            execution_status = exec_response.get("status")
            print_info(f"  Execution status: {execution_status} (waited {wait_time}s)")
            if execution_status in ["completed", "failed"]:
                break
    
    if execution_status == "completed":
        print_success(f"Execution completed in {wait_time} seconds")
    elif execution_status == "failed":
        print_error(f"Execution failed after {wait_time} seconds")
        error_msg = exec_response.get("error_message", "Unknown error")
        print_error(f"  Error: {error_msg}")
        return False
    else:
        print_warning(f"Execution still {execution_status} after {wait_time} seconds")
        print_warning("  Continuing with validation anyway...")
    
    # Step 5: Get Results Summary
    print_info("Step 5: Getting Results Summary...")
    summary = test_api_endpoint("GET", f"/results/execution/{execution_id}/summary")
    if summary:
        print_success(f"Total Results: {summary.get('total_results', 0)}")
        print_success(f"Safety Score: {summary.get('safety_score', 'N/A')}")
        print_success(f"Safety Grade: {summary.get('safety_grade', 'N/A')}")
        if summary.get('by_severity'):
            for severity, count in summary['by_severity'].items():
                if count > 0:
                    print_info(f"  {severity}: {count}")
    else:
        print_warning("Could not get results summary (may be normal if execution is still running)")
    
    # Step 6: Create Baseline
    print_info("Step 6: Creating Baseline...")
    baseline_data = {
        "execution_id": execution_id,
        "name": "Validation Test Baseline",
        "description": "Baseline created during validation",
        "baseline_tag": "validation-test"
    }
    
    baseline = test_api_endpoint("POST", "/baselines", baseline_data)
    if baseline:
        baseline_id = baseline.get("id")
        print_success(f"Created Baseline ID: {baseline_id}")
    else:
        print_warning("Could not create baseline (execution may not be completed)")
        baseline_id = None
    
    # Step 7: Create Second Execution for Drift Comparison
    if execution_status == "completed":
        print_info("Step 7: Creating second execution for drift comparison...")
        execution2 = test_api_endpoint("POST", "/executions/", execution_data)
        if execution2:
            execution2_id = execution2.get("id")
            print_success(f"Created Execution 2 ID: {execution2_id}")
            
            # Wait for second execution
            print_info("  Waiting for second execution to complete...")
            wait_time2 = 0
            while wait_time2 < max_wait:
                time.sleep(5)
                wait_time2 += 5
                exec2_response = test_api_endpoint("GET", f"/executions/{execution2_id}")
                if exec2_response:
                    exec2_status = exec2_response.get("status")
                    if exec2_status in ["completed", "failed"]:
                        break
            
            if exec2_status == "completed":
                print_success(f"Execution 2 completed in {wait_time2} seconds")
                
                # Step 8: Compare for Drift
                print_info("Step 8: Comparing executions for drift...")
                if baseline_id:
                    drift_data = {
                        "execution_id": execution2_id,
                        "baseline_execution_id": execution_id
                    }
                else:
                    drift_data = {
                        "execution_id": execution2_id,
                        "baseline_mode": "previous"
                    }
                
                drift_response = test_api_endpoint("POST", "/drift/compare", drift_data)
                if drift_response:
                    print_success("Drift comparison initiated")
                    drift_score = drift_response.get("drift_score", "N/A")
                    drift_grade = drift_response.get("drift_grade", "N/A")
                    print_success(f"Drift Score: {drift_score}")
                    print_success(f"Drift Grade: {drift_grade}")
                    
                    # Wait a bit for drift processing
                    time.sleep(3)
                    
                    # Get drift summary
                    drift_summary = test_api_endpoint("GET", f"/drift/execution/{execution2_id}/summary")
                    if drift_summary:
                        print_success(f"Total Drift Results: {drift_summary.get('total_drift_results', 0)}")
                else:
                    print_warning("Drift comparison failed or not available")
            else:
                print_warning("Second execution did not complete in time")
        else:
            print_warning("Could not create second execution")
    
    return True

def validate_library_adapters():
    """Validate library adapters"""
    print_header("7. Library Adapters Validation")
    try:
        from app.services.library_adapters import (
            GarakAdapter, PyRITAdapter, LangTestAdapter, PromptfooAdapter
        )
        
        adapters = [
            ("Garak", GarakAdapter),
            ("PyRIT", PyRITAdapter),
            ("LangTest", LangTestAdapter),
            ("Promptfoo", PromptfooAdapter),
        ]
        
        all_ok = True
        for name, adapter_class in adapters:
            try:
                adapter = adapter_class()
                adapter_name = adapter.get_name()
                print_success(f"{name} adapter: {adapter_name}")
                
                # Check if it supports categories
                if hasattr(adapter, 'supports_category'):
                    test_categories = ["prompt_injection", "jailbreak", "bias", "robustness"]
                    supported = [cat for cat in test_categories if adapter.supports_category(cat)]
                    if supported:
                        print_info(f"  Supports: {', '.join(supported[:3])}")
            except Exception as e:
                print_error(f"{name} adapter failed: {e}")
                all_ok = False
        
        return all_ok
    except Exception as e:
        print_error(f"Library adapters validation failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def validate_data_integrity():
    """Validate data integrity"""
    print_header("8. Data Integrity Validation")
    try:
        from app.db.database import SessionLocal
        from app.db.models import Baseline, Embedding, DriftResult, AgentTrace, Execution, Result
        
        db = SessionLocal()
        all_ok = True
        
        try:
            # Check baselines
            baselines = db.query(Baseline).all()
            for b in baselines:
                exec = db.query(Execution).filter(Execution.id == b.execution_id).first()
                if not exec:
                    print_error(f"Baseline {b.id} references missing execution {b.execution_id}")
                    all_ok = False
            
            if baselines:
                print_success(f"Validated {len(baselines)} baselines")
            else:
                print_warning("No baselines found (normal for new installation)")
            
            # Check embeddings
            embeddings = db.query(Embedding).limit(10).all()
            for e in embeddings:
                if not e.embedding_vector or len(e.embedding_vector) == 0:
                    print_error(f"Embedding {e.id} has empty vector")
                    all_ok = False
                elif not isinstance(e.embedding_vector, list):
                    print_error(f"Embedding {e.id} vector is not a list")
                    all_ok = False
            
            if embeddings:
                print_success(f"Validated {len(embeddings)} embeddings (sample)")
            else:
                print_warning("No embeddings found (will be generated after execution)")
            
            # Check drift results
            drift_results = db.query(DriftResult).limit(10).all()
            for d in drift_results:
                if d.value < -1 or d.value > 2:  # Allow some range for metrics
                    print_warning(f"DriftResult {d.id} has unusual value: {d.value}")
            
            if drift_results:
                print_success(f"Validated {len(drift_results)} drift results (sample)")
            else:
                print_warning("No drift results found (run comparison to generate)")
            
            # Check executions and results
            executions = db.query(Execution).limit(5).all()
            if executions:
                print_success(f"Found {len(executions)} executions (sample)")
                for exec in executions:
                    results_count = db.query(Result).filter(Result.execution_id == exec.id).count()
                    if results_count > 0:
                        print_info(f"  Execution {exec.id}: {results_count} results")
            else:
                print_warning("No executions found")
            
        finally:
            db.close()
        
        return all_ok
    except Exception as e:
        print_error(f"Data integrity check failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_agent_traces():
    """Test agent trace extraction and functionality"""
    print_header("9. Agent Traces Validation")
    try:
        from app.db.database import SessionLocal
        from app.db.models import Execution, Result, AgentTrace
        from app.services.agent_trace_extractor import AgentTraceExtractor
        from app.core.config import settings
        
        db = SessionLocal()
        all_ok = True
        
        try:
            # Check if agent traces are enabled
            if not settings.ENABLE_AGENT_TRACES:
                print_warning("ENABLE_AGENT_TRACES is False - agent traces are disabled")
                print_info("  Set ENABLE_AGENT_TRACES = True in config to enable")
            
            # Find a completed execution
            execution = db.query(Execution).filter(Execution.status == "completed").first()
            if not execution:
                print_warning("No completed execution found - skipping agent trace test")
                print_info("  Run an execution first to test agent traces")
                return True  # Don't fail if no executions exist
            
            execution_id = execution.id
            print_info(f"Testing with execution ID: {execution_id}")
            
            # Check if traces already exist
            existing_traces = db.query(AgentTrace).filter(AgentTrace.execution_id == execution_id).all()
            if existing_traces:
                print_success(f"Found {len(existing_traces)} existing traces")
                for trace in existing_traces[:3]:  # Show first 3
                    print_info(f"  Step {trace.step}: {trace.action_type} ({trace.tool_name or 'N/A'})")
            else:
                # Try to extract traces from metadata
                print_info("No existing traces found, attempting extraction from metadata...")
                
                # Add test trace data to a result if none exists
                result = db.query(Result).filter(Result.execution_id == execution_id).first()
                if result:
                    if not result.extra_metadata:
                        result.extra_metadata = {}
                    
                    # Add test agent trace if not present
                    if "agent_trace" not in result.extra_metadata:
                        result.extra_metadata["agent_trace"] = {
                            "action_type": "tool_call",
                            "tool_name": "test_tool",
                            "metadata": {
                                "function_name": "test_function",
                                "step": 1
                            }
                        }
                        db.commit()
                        print_info("Added test agent trace to result metadata")
                    
                    # Extract traces
                    traces = AgentTraceExtractor.extract_traces_from_execution(execution_id)
                    if traces:
                        print_success(f"Extracted {len(traces)} traces from metadata")
                        for trace in traces[:3]:
                            print_info(f"  Step {trace.step}: {trace.action_type} ({trace.tool_name or 'N/A'})")
                    else:
                        print_warning("No traces extracted (may be normal if no agent trace data in metadata)")
                else:
                    print_warning("No results found for execution - cannot test trace extraction")
            
            # Test get_traces_for_execution
            all_traces = AgentTraceExtractor.get_traces_for_execution(execution_id)
            print_success(f"get_traces_for_execution() returned {len(all_traces)} traces")
            
            # Test parse_tool_call
            test_tool_data = {
                "execution_id": execution_id,
                "step": 1,
                "action_type": "tool_call",
                "tool_name": "test_parse",
                "metadata": {"test": "data"}
            }
            parsed_trace = AgentTraceExtractor.parse_tool_call(test_tool_data)
            if parsed_trace:
                print_success("parse_tool_call() works correctly")
            
            return all_ok
            
        finally:
            db.close()
            
    except Exception as e:
        print_error(f"Agent traces validation failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_embeddings():
    """Test embedding generation and functionality"""
    print_header("10. Embeddings Validation")
    try:
        from app.db.database import SessionLocal
        from app.db.models import Execution, Result, Embedding
        from app.services.embedding_generator import EmbeddingGenerator
        from app.core.config import settings
        
        db = SessionLocal()
        all_ok = True
        
        try:
            # Check embedding service configuration
            embedding_url = settings.EMBEDDING_SERVICE_URL
            model_name = settings.EMBEDDING_MODEL_NAME
            print_success(f"Embedding service URL: {embedding_url}")
            print_success(f"Embedding model name: {model_name}")
            
            # Test embedding generation for a single text
            test_text = "This is a test response for embedding generation."
            try:
                embedding = EmbeddingGenerator.generate_embedding_for_text(test_text, model_name)
                if embedding and len(embedding) > 0:
                    print_success(f"Generated embedding vector (dimension: {len(embedding)})")
                    print_info(f"  First 5 values: {embedding[:5]}")
                else:
                    print_error("Failed to generate embedding - empty result")
                    all_ok = False
            except Exception as e:
                print_error(f"Failed to generate embedding: {e}")
                print_warning("  This may indicate the embedding service is not running")
                print_warning(f"  Check that embedding service is available at: {embedding_url}")
                all_ok = False
            
            # Check embeddings in database
            execution = db.query(Execution).filter(Execution.status == "completed").first()
            if execution:
                execution_id = execution.id
                print_info(f"Checking embeddings for execution ID: {execution_id}")
                
                # Get embeddings for this execution
                embeddings = EmbeddingGenerator.get_embeddings_for_execution(execution_id)
                if embeddings:
                    print_success(f"Found {len(embeddings)} embeddings for execution {execution_id}")
                    
                    # Validate embedding structure
                    for emb in embeddings[:3]:  # Check first 3
                        if emb.embedding_vector and len(emb.embedding_vector) > 0:
                            print_info(f"  Embedding {emb.id}: {len(emb.embedding_vector)} dimensions, model: {emb.model_name}")
                        else:
                            print_error(f"  Embedding {emb.id} has invalid vector")
                            all_ok = False
                    
                    # Test centroid computation
                    if len(embeddings) > 1:
                        vectors = [e.embedding_vector for e in embeddings]
                        centroid = EmbeddingGenerator.compute_centroid(vectors)
                        if centroid and len(centroid) > 0:
                            print_success(f"Computed centroid (dimension: {len(centroid)})")
                        else:
                            print_error("Failed to compute centroid")
                            all_ok = False
                        
                        # Test cosine similarity
                        if len(vectors) >= 2:
                            similarity = EmbeddingGenerator.cosine_similarity(vectors[0], vectors[1])
                            if 0 <= similarity <= 1:
                                print_success(f"Cosine similarity test: {similarity:.4f}")
                            else:
                                print_error(f"Invalid cosine similarity: {similarity}")
                                all_ok = False
                else:
                    print_warning("No embeddings found for execution")
                    print_info("  Embeddings are generated automatically after execution completes via API")
                    print_info("  This is normal if execution just completed or embedding service is unavailable")
            else:
                print_warning("No completed execution found - cannot test execution embeddings")
            
            return all_ok
            
        finally:
            db.close()
            
    except Exception as e:
        print_error(f"Embeddings validation failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_drift_detection_detailed():
    """Test detailed drift detection including agent traces and embeddings"""
    print_header("11. Detailed Drift Detection Validation")
    try:
        from app.db.database import SessionLocal
        from app.db.models import Execution, DriftResult, Embedding, AgentTrace, Result
        from app.services.drift_engine import DriftEngine
        from app.services.embedding_generator import EmbeddingGenerator
        from app.services.agent_trace_extractor import AgentTraceExtractor
        
        db = SessionLocal()
        all_ok = True
        
        try:
            # Find two completed executions for comparison
            executions = db.query(Execution).filter(Execution.status == "completed").limit(2).all()
            if len(executions) < 2:
                print_warning("Need at least 2 completed executions for drift testing")
                print_info("  Run multiple executions first")
                return True  # Don't fail if insufficient data
            
            exec1, exec2 = executions[0], executions[1]
            print_info(f"Comparing Execution {exec2.id} (current) vs Execution {exec1.id} (baseline)")
            
            # Test 1: Output Drift
            print_info("\n1. Testing Output Drift Detection...")
            try:
                results1 = db.query(Result).filter(Result.execution_id == exec1.id).all()
                results2 = db.query(Result).filter(Result.execution_id == exec2.id).all()
                
                if results1 and results2:
                    output_drift = DriftEngine.detect_output_drift(results2, results1)
                    if output_drift:
                        print_success(f"Output drift detected: {len(output_drift)} metrics")
                        for drift in output_drift:
                            print_info(f"  - {drift.metric}: {drift.value:.4f} ({drift.severity})")
                    else:
                        print_warning("No output drift detected (may be normal if responses are similar)")
                else:
                    print_warning("Insufficient results for output drift test")
            except Exception as e:
                print_error(f"Output drift test failed: {e}")
                all_ok = False
            
            # Test 2: Safety Drift
            print_info("\n2. Testing Safety Drift Detection...")
            try:
                safety_drift = DriftEngine.detect_safety_drift(exec2.id, exec1.id)
                if safety_drift:
                    print_success("Safety drift detected")
                    print_info(f"  - Metric: {safety_drift.metric}")
                    print_info(f"  - Value: {safety_drift.value:.4f}")
                    print_info(f"  - Severity: {safety_drift.severity}")
                else:
                    print_warning("No safety drift detected")
            except Exception as e:
                print_error(f"Safety drift test failed: {e}")
                all_ok = False
            
            # Test 3: Distribution Drift
            print_info("\n3. Testing Distribution Drift Detection...")
            try:
                results1 = db.query(Result).filter(Result.execution_id == exec1.id).all()
                results2 = db.query(Result).filter(Result.execution_id == exec2.id).all()
                if results1 and results2:
                    dist_drift = DriftEngine.detect_distribution_drift(results2, results1)
                    if dist_drift:
                        print_success(f"Distribution drift detected: {len(dist_drift)} metrics")
                        for drift in dist_drift:
                            print_info(f"  - {drift.metric}: {drift.value:.4f} ({drift.severity})")
                    else:
                        print_warning("No distribution drift detected")
                else:
                    print_warning("Insufficient results for distribution drift test")
            except Exception as e:
                print_error(f"Distribution drift test failed: {e}")
                all_ok = False
            
            # Test 4: Embedding Drift
            print_info("\n4. Testing Embedding Drift Detection...")
            try:
                # Check if embeddings exist
                emb1 = EmbeddingGenerator.get_embeddings_for_execution(exec1.id)
                emb2 = EmbeddingGenerator.get_embeddings_for_execution(exec2.id)
                
                if emb1 and emb2:
                    embedding_drift = DriftEngine.detect_embedding_drift(exec2.id, exec1.id)
                    if embedding_drift:
                        print_success("Embedding drift detected")
                        print_info(f"  - Metric: {embedding_drift.metric}")
                        print_info(f"  - Value: {embedding_drift.value:.4f}")
                        print_info(f"  - Severity: {embedding_drift.severity}")
                        if embedding_drift.details:
                            cosine_sim = embedding_drift.details.get('cosine_similarity')
                            if cosine_sim is not None:
                                print_info(f"  - Cosine Similarity: {cosine_sim:.4f}")
                    else:
                        print_warning("No embedding drift detected (embeddings may be very similar)")
                else:
                    print_warning("Embeddings not found for one or both executions")
                    print_info("  Embeddings are generated automatically after execution completes")
                    print_info("  Wait a few minutes after execution completion for embeddings to be generated")
            except Exception as e:
                print_error(f"Embedding drift test failed: {e}")
                all_ok = False
            
            # Test 5: Agent/Tool Drift
            print_info("\n5. Testing Agent/Tool Drift Detection...")
            try:
                # Check if agent traces exist
                traces1 = AgentTraceExtractor.get_traces_for_execution(exec1.id)
                traces2 = AgentTraceExtractor.get_traces_for_execution(exec2.id)
                
                if traces1 and traces2:
                    agent_drift = DriftEngine.detect_agent_tool_drift(exec2.id, exec1.id)
                    if agent_drift:
                        print_success(f"Agent/tool drift detected: {len(agent_drift)} metrics")
                        for drift in agent_drift:
                            print_info(f"  - {drift.metric}: {drift.value:.4f} ({drift.severity})")
                            if drift.details:
                                print_info(f"    Details: {list(drift.details.keys())}")
                    else:
                        print_warning("No agent/tool drift detected (traces may be similar)")
                else:
                    print_warning("Agent traces not found for one or both executions")
                    print_info("  Agent traces are extracted from result metadata")
                    print_info("  Include 'agent_trace' in result.extra_metadata to enable")
            except Exception as e:
                print_error(f"Agent/tool drift test failed: {e}")
                all_ok = False
            
            # Test 6: Full Drift Comparison
            print_info("\n6. Testing Full Drift Comparison...")
            try:
                drift_results = DriftEngine.compare_executions(
                    current_execution_id=exec2.id,
                    baseline_execution_id=exec1.id,
                    drift_types=["output", "safety", "distribution", "embedding", "agent_tool"]
                )
                
                if drift_results:
                    print_success(f"Full comparison completed: {len(drift_results)} drift results")
                    
                    # Group by type
                    by_type = {}
                    for drift in drift_results:
                        drift_type = drift.drift_type
                        by_type[drift_type] = by_type.get(drift_type, 0) + 1
                    
                    for drift_type, count in by_type.items():
                        print_info(f"  - {drift_type}: {count} result(s)")
                    
                    # Calculate drift score
                    drift_score = DriftEngine.calculate_drift_score(drift_results)
                    drift_grade = DriftEngine.get_drift_grade(drift_score)
                    print_success(f"Drift Score: {drift_score:.2f} (Grade: {drift_grade})")
                else:
                    print_warning("No drift results from full comparison")
                    print_info("  This may be normal if executions are very similar")
            except Exception as e:
                print_error(f"Full drift comparison failed: {e}")
                import traceback
                traceback.print_exc()
                all_ok = False
            
            return all_ok
            
        finally:
            db.close()
            
    except Exception as e:
        print_error(f"Detailed drift detection validation failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print_header("PromptShield v1.1 Comprehensive Validation")
    print_info(f"API Base URL: {API_BASE}")
    print_info(f"Python Version: {sys.version}")
    print()
    
    # Check server first
    if not check_server():
        print_error("\nCannot proceed without backend server. Please start it first.")
        sys.exit(1)
    
    # Run all validations
    results = {
        "Database": validate_database(),
        "Models": validate_models(),
        "Services": validate_services(),
        "Configuration": validate_config(),
        "API Endpoints": validate_api_endpoints(),
        "Library Adapters": validate_library_adapters(),
        "Data Integrity": validate_data_integrity(),
        "Agent Traces": test_agent_traces(),
        "Embeddings": test_embeddings(),
        "Detailed Drift Detection": test_drift_detection_detailed(),
    }
    
    # Test full workflow
    workflow_result = test_full_workflow()
    results["Full Workflow"] = workflow_result
    
    # Summary
    print_header("Validation Summary")
    
    passed = sum(1 for v in results.values() if v)
    total = len(results)
    
    for name, result in results.items():
        if result:
            print_success(f"{name}: PASSED")
        else:
            print_error(f"{name}: FAILED")
    
    print()
    print(f"{Colors.BOLD}Results: {passed}/{total} validations passed{Colors.RESET}")
    
    if passed == total:
        print_success("\nðŸŽ‰ All validations passed!")
        print_info("\nNext steps:")
        print_info("  1. Check the UI at http://localhost:3000")
        print_info("  2. Create more executions with different pipelines")
        print_info("  3. Create baselines and compare for drift")
        print_info("  4. Review results and safety scores")
        print_info("  5. Check agent traces and embeddings in the database")
        sys.exit(0)
    else:
        print_error(f"\nâš ï¸  {total - passed} validation(s) failed")
        print_info("Please review the errors above and fix them before proceeding.")
        sys.exit(1)

if __name__ == "__main__":
    main()
